{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Resumir textos grandes con Langchain",
   "id": "3798fe2b7593ca4d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Instalación de paquetes\n",
    "Si estás corriendo este notebook en Google Colab, corre la siguiente celda para instalar los paquetes necesarios."
   ],
   "id": "26f4ce426cf6c1a1"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-11-24T14:10:15.960830Z",
     "start_time": "2024-11-24T14:10:15.956757Z"
    }
   },
   "source": "# %pip install langchain langchain_community langchain_openai",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-24T14:10:16.003007Z",
     "start_time": "2024-11-24T14:10:15.984491Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Corre esta celda solo si tienes un archivo .env configurado\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ],
   "id": "4b38b57c7efc3306",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Definición de Contexto\n",
    "\n",
    "En el ámbito de los LLMs, la \"ventana de contexto\" representa el tamaño máximo del prompt proporcionado a un LLM, que incluye instrucciones y contexto. Diferentes LLMs tienen límites de tokens distintos para la ventana de contexto: GPT-3.5 solía aceptar hasta 16K tokens, GPT-4o y LLama 3 hasta 128K, y Gemini-1.5 hasta 1M."
   ],
   "id": "6b5ac8e5d19057b7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Resumir un documento grande",
   "id": "62e94ee3f62359e3"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Vamos a resumir el libro \"Cien años de soledad\" de Gabriel García Márquez. Que puedes descargar en el siguiente enlace (en formato txt y cortado a 18.000 palabras): [Cien años de soledad](https://gist.githubusercontent.com/ismaproco/6781d297ee65c6a707cd3c901e87ec56/raw/20d3520cd7c53d99215845375b1dca16ac827bd7/gabriel_garcia_marquez_cien_annos_soledad.txt)\n",
    "\n",
    "Lo que vamos a realizar de manera simplificada es lo siguiente:\n",
    "\n",
    "1. Dividir el texto grandes en documentos pequeños (3000 tokens)\n",
    "2. Generar resúmenes en paralelo de cada documento pequeño (operación Map)\n",
    "3. Reducir todos los resúmenes a un solo resumen (operación Reduce)\n",
    "\n",
    "Esta técnica es conocida como ***MapReduce***."
   ],
   "id": "4b45aed334f211b0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-24T14:10:37.354359Z",
     "start_time": "2024-11-24T14:10:37.351351Z"
    }
   },
   "cell_type": "code",
   "source": [
    "with open(\"../../datasets/cien_annos_soledad_reducido.txt\", 'r', encoding='utf-8') as f:\n",
    "    book = f.read()"
   ],
   "id": "e13c69a6438204ef",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-24T14:10:44.466168Z",
     "start_time": "2024-11-24T14:10:42.703456Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Importa las librerías necesarias\n",
    "\n",
    "import os\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_text_splitters import TokenTextSplitter\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnableLambda, RunnableParallel\n",
    "import getpass"
   ],
   "id": "59aab95ff2cc493a",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-24T14:10:44.566372Z",
     "start_time": "2024-11-24T14:10:44.478689Z"
    }
   },
   "cell_type": "code",
   "source": [
    "llm = ChatOpenAI(\n",
    "    model=os.getenv(\"MODEL\"),\n",
    "    openai_api_key=os.getenv(\"LIA_API_KEY\"),\n",
    "    openai_api_base=os.getenv(\"LIA_API_BASE\"),\n",
    "    max_tokens=2000,\n",
    "    temperature=0.6,\n",
    ")"
   ],
   "id": "589d8e470d4776f4",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-24T14:10:44.578837Z",
     "start_time": "2024-11-24T14:10:44.574381Z"
    }
   },
   "cell_type": "code",
   "source": [
    "text_chunks_chain = (\n",
    "    RunnableLambda(lambda x: \n",
    "        [\n",
    "            {\n",
    "                'chunk': text_chunk, \n",
    "            }\n",
    "            for text_chunk in \n",
    "               TokenTextSplitter(chunk_size=3000, chunk_overlap=100).split_text(x)\n",
    "        ]\n",
    "    )\n",
    ")"
   ],
   "id": "2be2ffdb52fdf79d",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-24T14:10:44.591170Z",
     "start_time": "2024-11-24T14:10:44.588137Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Es lo mismo que si escribiéramos:\n",
    "\n",
    "def split_into_chunks(text):\n",
    "    \n",
    "    # Crear un divisor de texto que usa los parámetros chunk_size y chunk_overlap\n",
    "    splitter = TokenTextSplitter(chunk_size=3000, chunk_overlap=100)\n",
    "    \n",
    "    # Dividir el texto en fragmentos\n",
    "    chunks = splitter.split_text(text)\n",
    "    \n",
    "    # Devolver cada fragmento como un diccionario\n",
    "    return [{'chunk': text_chunk} for text_chunk in chunks]\n",
    "\n",
    "# Convertir la función en un componente 'Runnable' usando RunnableLambda\n",
    "text_chunks_chain = RunnableLambda(split_into_chunks)"
   ],
   "id": "250a88a1384e4cd6",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-24T14:10:44.749469Z",
     "start_time": "2024-11-24T14:10:44.745151Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# El siguiente paso es configurar la cadena de mapeo, la cual ejecutará una solicitud de resumen para cada fragmento de documento:\n",
    "\n",
    "\n",
    "plantilla_resumir_fragmento = \"\"\"\n",
    "Escribe un resumen conciso del siguiente texto, e incluye los detalles principales.\n",
    "Texto: {chunk}\n",
    "\"\"\"\n",
    " \n",
    "resumir_fragmento_prompt = PromptTemplate.from_template(plantilla_resumir_fragmento)\n",
    "cadena_resumir_fragmento = resumir_fragmento_prompt | llm\n",
    " \n",
    "cadena_mapa_resumir = (\n",
    "    RunnableParallel (\n",
    "        {\n",
    "            'resumen': cadena_resumir_fragmento | StrOutputParser()     \n",
    "        }\n",
    "    )\n",
    ")\n"
   ],
   "id": "58fac34111c291f3",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Ahora vamos a configurar la cadena de reducción (reduce chain), que resume los resúmenes de cada fragmento del documento, sigue un proceso similar al de la cadena de mapeo, pero requiere un poco más de configuración.",
   "id": "f20cfd63e4f0e442"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-24T14:10:45.379448Z",
     "start_time": "2024-11-24T14:10:45.376889Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Creamos la plantilla de reducción\n",
    "\n",
    "plantilla_resumir_resúmenes = \"\"\"\n",
    "Escribe un resumen conciso del siguiente texto, que combina varios resúmenes, e incluye los detalles principales.\n",
    "Texto: {resumenes}\n",
    "\"\"\"\n",
    " \n",
    "resumir_resumenes_prompt = PromptTemplate.from_template(plantilla_resumir_resúmenes)\n"
   ],
   "id": "d3e70631b326b1df",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-24T14:10:45.700938Z",
     "start_time": "2024-11-24T14:10:45.697145Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Ahora, vamos a configurar la cadena de reducción (reduce chain), que sintetiza en uno los resúmenes de cada fragmento del documento\n",
    "\n",
    "cadena_reducir_resumenes = (\n",
    "    RunnableLambda(lambda x: \n",
    "        {\n",
    "            'resumenes': '\\n'.join([i['resumen'] for i in x]), \n",
    "        })\n",
    "    | resumir_resumenes_prompt \n",
    "    | llm \n",
    "    | StrOutputParser()\n",
    ")"
   ],
   "id": "856a9019bf447d82",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-24T14:10:46.363263Z",
     "start_time": "2024-11-24T14:10:46.360515Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Finalmente, vamos a encadenar todos los componentes para crear el pipeline completo\n",
    "\n",
    "cadena_map_reduce = (\n",
    "   text_chunks_chain\n",
    "   | cadena_mapa_resumir.map()\n",
    "   | cadena_reducir_resumenes\n",
    ") \n",
    "\n",
    "# La función map() en cadena_mapa_resumir es esencial para habilitar el procesamiento paralelo de los fragmentos."
   ],
   "id": "8e958c6c4f7a3fa6",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-24T14:11:17.258809Z",
     "start_time": "2024-11-24T14:11:06.348918Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Ahora vamos a correr la cadena\n",
    "\n",
    "resumen_final = cadena_map_reduce.invoke(book)"
   ],
   "id": "557800bcbdbd9586",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-24T14:11:17.266377Z",
     "start_time": "2024-11-24T14:11:17.263765Z"
    }
   },
   "cell_type": "code",
   "source": "print(resumen_final)",
   "id": "570563dabdcb09cd",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "En \"Cien años de soledad\" de Gabriel García Márquez, la historia gira en torno a la familia Buendía en la aldea de Macondo, fundada por José Arcadio Buendía y su esposa Úrsula. A lo largo del relato, José Arcadio se obsesiona con el conocimiento y la ciencia, impulsado por la llegada de gitanos como Melquíades, quien le introduce inventos fascinantes. Su búsqueda de la alquimia y la transformación de metales en oro resulta en fracasos y descuidos familiares, afectando su relación con Úrsula y sus hijos, especialmente Aureliano, quien muestra un talento natural.\n",
      "\n",
      "La narrativa también explora la vida en Macondo, desde su fundación hasta la llegada de nuevos personajes como Rebeca, una niña huérfana con costumbres peculiares, y la epidemia de insomnio que amenaza a la familia, provocando el olvido de su identidad. A medida que José Arcadio se sumerge en su obsesión, su familia enfrenta tensiones y desafíos, reflejando la lucha entre el deseo de conocimiento y las responsabilidades familiares.\n",
      "\n",
      "El relato destaca las complejidades de las relaciones humanas, el impacto del pasado y la búsqueda de conexión en un mundo mágico y a menudo caótico. La historia de Macondo es un ciclo de creación y destrucción, simbolizando la soledad inherente a la búsqueda de la verdad y el conocimiento en un entorno lleno de realidades mágicas.\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "403ef460f41bde28"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
