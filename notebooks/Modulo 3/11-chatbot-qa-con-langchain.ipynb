{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Chatbot QA con LangChain (y LangSmith)",
   "id": "3625efd6ac61ccd2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "![Clases de Langchain vinculadas a la etapa de ingesta de contenidos](https://github.com/lia-ve/agentes-langchain/blob/main/img/langchain_ingestion.jpg?raw=true)",
   "id": "6e9c4f00acb02564"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Aquí están los componentes mostrados en la figura:\n",
    "\n",
    "- **Document**: Modela el contenido del texto y los metadatos relacionados.\n",
    "- **BaseLoader**: Carga texto desde fuentes externas en el modelo de documento.\n",
    "- **TextSplitter (Divisor de Texto)**: Divide los documentos en fragmentos más pequeños para un procesamiento eficiente.\n",
    "- **VectorStore**: Almacena fragmentos de texto y sus embeddings relacionados para una recuperación eficiente.\n",
    "- **Embeddings**: Convierte el texto en embeddings (representaciones vectoriales)."
   ],
   "id": "3410d8dfb29ad4c4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "![Clases de Langchain vinculadas a la etapa de recuperación y generación](https://github.com/lia-ve/agentes-langchain/blob/main/img/langchain_gen.jpg?raw=true)",
   "id": "f7348cb258fc7430"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "La figura incluye los siguientes componentes:\n",
    "\n",
    "- **Vector Store**: Almacena y recupera fragmentos de texto relevantes.\n",
    "- **Retriever**: Recupera fragmentos de texto relevantes basándose en la similitud entre el embedding de la consulta y los embeddings de texto almacenados.\n",
    "- **Embedding Model**: Asegura embeddings consistentes para las consultas y documentos.\n",
    "- **Prompt / PromptTemplate**: Construye la entrada para el modelo de lenguaje, utilizando típicamente la pregunta del usuario y un contexto formado por los fragmentos de texto recuperados.\n",
    "- **Chat Model / LLM**: Genera respuestas utilizando el contexto y la consulta proporcionados."
   ],
   "id": "cf5fa7ad690939dd"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": "# %pip install langchain langchain_community langchain-chroma langchain_openai langchain-unstructured chromadb docx2txt pypdf wikipedia unstructured",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Corre esta celda solo si tienes un archivo .env configurado\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ],
   "id": "ef59300bd44da4db",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##  Ingesta de Contenidos",
   "id": "6163aab5ba8d3da8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "\n",
    "from langchain_community.document_loaders import WikipediaLoader, Docx2txtLoader, PyPDFLoader, TextLoader\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings"
   ],
   "id": "8f4cf92cae43070",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "embeddings_model = OpenAIEmbeddings(openai_api_key=os.getenv(\"LIA_API_KEY\"),openai_api_base=os.getenv(\"LIA_EMBEDDING_API_BASE\"), model=os.getenv(\"EMBEDDING_MODEL\"))\n",
    "vector_db = Chroma(\"info_anu\", embeddings_model)"
   ],
   "id": "693ac55cb44a12b3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "### Carga de Documentos\n",
    "\n",
    "Procesa cada documento cargándolo, dividiéndolo en fragmentos, convirtiendo los fragmentos en vectores y guardando en la base de datos vectorial."
   ],
   "id": "c5c8f656fba58335"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Hagamos una utility para dividir y cargar documentos\n",
    "\n",
    "def split_and_import(loader):\n",
    "     chunks = text_splitter.split_documents(loader.load())\n",
    "     vector_db.add_documents(chunks)\n",
    "     print(f\"Fragmentos cargados de la fuente {loader}\")"
   ],
   "id": "2943a02525a95c9b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Carga de documentos de Wikipedia\n",
    "\n",
    "wikipedia_loader = WikipediaLoader(query=\"Pueblo Añú\")\n",
    "split_and_import(wikipedia_loader)"
   ],
   "id": "9e23468b880c5283",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Carga de documentos de otros formatos\n",
    "\n",
    "word_loader = Docx2txtLoader(\"../../datasets/anu/pueblo_anu.docx\")\n",
    "split_and_import(word_loader)\n",
    " \n",
    "pdf_loader = PyPDFLoader(\"../../datasets/anu/pautas-crianza-pueblo-anu-venezuela-completo.pdf\")\n",
    "split_and_import(pdf_loader)\n",
    " \n",
    "pdf_loader = PyPDFLoader(\"../../datasets/anu/tesis-anu-reducido.pdf\")\n",
    "split_and_import(pdf_loader)\n",
    " \n",
    "txt_loader = TextLoader(\"../../datasets/anu/pueblo_anu.txt\")\n",
    "split_and_import(txt_loader)"
   ],
   "id": "e2cdd2666c92217a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Cargar documentos desde un directorio",
   "id": "8bdcae5cba6d6895"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Mapeo de extensiones de archivo a clases de cargadores de documentos\n",
    "loader_classes = {\n",
    "    'docx': Docx2txtLoader,\n",
    "    'pdf': PyPDFLoader,\n",
    "    'txt': TextLoader\n",
    "}\n",
    "\n",
    "# Clase para cargar todos los documentos en un directorio\n",
    "class DirectoryLoader:\n",
    "    def __init__(self, folder_path, vector_db):\n",
    "        self.folder_path = folder_path\n",
    "        self.vector_db = vector_db\n",
    "    \n",
    "    def get_loader(self, filename):\n",
    "        \"\"\"Devuelve el cargador apropiado para el archivo basado en su extensión.\"\"\"\n",
    "        _, file_extension = os.path.splitext(filename)\n",
    "        file_extension = file_extension.lstrip('.')\n",
    "        \n",
    "        loader_class = loader_classes.get(file_extension)\n",
    "        \n",
    "        if loader_class:\n",
    "            return loader_class(filename)\n",
    "        else:\n",
    "            raise ValueError(f\"No hay cargador disponible para la extensión de archivo '{file_extension}'\")\n",
    "\n",
    "    def load_all_documents(self):\n",
    "        \"\"\"Carga todos los documentos compatibles en el directorio y los agrega a la base de datos vectorial.\"\"\"\n",
    "        for filename in os.listdir(self.folder_path):\n",
    "            file_path = os.path.join(self.folder_path, filename)\n",
    "            if os.path.isfile(file_path):  # Asegurarse de que es un archivo\n",
    "                try:\n",
    "                    loader = self.get_loader(file_path)  # Obtener el cargador apropiado\n",
    "                    chunks = text_splitter.split_documents(loader.load())\n",
    "                    self.vector_db.add_documents(chunks)\n",
    "                    print(f\"Fragmentos cargados de la fuente {filename}\")\n",
    "                except ValueError as e:\n",
    "                    print(e)  # Manejar extensiones de archivo no compatibles"
   ],
   "id": "243e79b6ade1644a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Uso de DirectoryLoader para cargar todos los documentos dentro de una carpeta\n",
    "\n",
    "directory_loader = DirectoryLoader(\"../../datasets/anu\", vector_db)\n",
    "directory_loader.load_all_documents()"
   ],
   "id": "7cd34bc0c8a8b153",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Realicemos una prueba de búsqueda en la base de datos vectorial\n",
    "\n",
    "query = \"Cómo son las viviendas Añú?\"\n",
    "results = vector_db.similarity_search(query, 4)\n",
    "print(results)"
   ],
   "id": "2995746e23e10a2e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Realizar una pregunta con una Cadena",
   "id": "8c68c9a60d8659ee"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "![Cadena de RAG](https://github.com/lia-ve/agentes-langchain/blob/main/img/rag_chain.png?raw=true)",
   "id": "e1383302417b1260"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Componentes de la cadena RAG:\n",
    "\n",
    "- **Retriever**: Recupera contenido de texto relevante de la base de datos vectorial y lo inyecta en el parámetro \"contexto\" del prompt.\n",
    "- **Alimentador de Preguntas**: Implementado como un componente de \"paso directo\", pasa la pregunta del usuario a través de la interfaz `Runnable` (una clase abstracta en la que se basa cada componente de LangChain).\n",
    "- **Modelo de Chat**: Procesa el prompt para generar la respuesta."
   ],
   "id": "6d946a7ff0cf4c9d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Primero, definamos la plantilla\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "rag_prompt_template = \"\"\"Utiliza las siguientes piezas de contexto para responder la pregunta al final.\n",
    "Si no sabes la respuesta, simplemente di que no sabes; no intentes inventar una respuesta.\n",
    "Usa un máximo de tres oraciones y mantén la respuesta lo más concisa posible.\n",
    "{context}\n",
    "Pregunta: {question}\n",
    "Respuesta útil:\"\"\"\n",
    "\n",
    "rag_prompt = PromptTemplate.from_template(rag_prompt_template)"
   ],
   "id": "d7fe1f91fca32210",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Alternativamente puedes una plantilla de LangChain Hub. El problema es que hay pocas en español.\n",
    "# from langchain import hub\n",
    "# rag_prompt = hub.pull(\"rlm/rag-prompt\")"
   ],
   "id": "453ab0a3be317443",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "retriever = vector_db.as_retriever()\n",
    " \n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "question_feeder = RunnablePassthrough()\n",
    " \n",
    "from langchain_openai import ChatOpenAI\n",
    "llm = ChatOpenAI(\n",
    "    model=os.getenv(\"MODEL\"),\n",
    "    openai_api_key=os.getenv(\"LIA_API_KEY\"),\n",
    "    openai_api_base=os.getenv(\"LIA_API_BASE\"),\n",
    ")"
   ],
   "id": "7a019c1870b5852b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "rag_chain = {\"context\": retriever, \"question\": question_feeder} | rag_prompt | llm",
   "id": "9b84bb49e25f16ee",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def execute_chain(chain, question):\n",
    "    answer = chain.invoke(question)\n",
    "    return answer.content"
   ],
   "id": "859e0a320105291c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Probe la cadena con una pregunta\n",
    "\n",
    "question = \"Qué tipo de viviendas usan el pueblo añú?\"\n",
    "answer = execute_chain(rag_chain, question)\n",
    "print(answer)"
   ],
   "id": "92732d84394f0d77",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Nuestro chatbot no tiene memoria\n",
    "\n",
    "question = \"Repite tu respuesta anterior\"\n",
    "answer = execute_chain(rag_chain, question)\n",
    "print(answer)"
   ],
   "id": "e8cdfa0eefe07155",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Memoria",
   "id": "3e5ebae3c658144b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "rag_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"Eres un asistente útil, experto en poblaciones indígenas venezolanas, especialmente en las comunidades ubicadas en la región occidental de Venezuela. Si no sabes la respuesta, simplemente di que no sabes; no intentes inventar una respuesta.\"),\n",
    "        (\"placeholder\", \"{chat_history_messages}\"),\n",
    "        (\"assistant\", \"{retrieved_context}\"),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")"
   ],
   "id": "e4cc4da199eceb6d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "chat_history_memory = ChatMessageHistory()"
   ],
   "id": "1c72eef58de7961b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Actualicemos nuestra función de ejecución de la cadena para incluir la memoria\n",
    "\n",
    "def execute_chain_with_memory(chain, question):\n",
    "    chat_history_memory.add_user_message(question)\n",
    "    answer = chain.invoke(question)\n",
    "    chat_history_memory.add_ai_message(answer)\n",
    "    return answer.content"
   ],
   "id": "453df6cc55d013f5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Hacemos esta función porque langchain debe recibir un objeto RunnableLambda\n",
    "\n",
    "def get_messages(x):\n",
    "    return chat_history_memory.messages"
   ],
   "id": "411c8eece40f4ed5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "rag_chain_memory = {\n",
    "    \"retrieved_context\": retriever, \n",
    "    \"question\": question_feeder,\n",
    "    \"chat_history_messages\": RunnableLambda(get_messages)\n",
    "} | rag_prompt | llm"
   ],
   "id": "88f7b5a64025f4aa",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Realicemos las preguntas de nuevo con la nueva cadena\n",
    "question = \"Qué tipo de viviendas usan el pueblo añú?\"\n",
    "answer = execute_chain_with_memory(rag_chain_memory, question)\n",
    "print(answer)"
   ],
   "id": "e51694d3266de468",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "question = \"Repite tu respuesta anterior\"\n",
    "answer = execute_chain(rag_chain_memory, question)\n",
    "print(answer)"
   ],
   "id": "1770673384ea174e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Monitorear la solución con LangSmith",
   "id": "c40f0b3cd382b372"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Definir estas variables de entorno en un archivo .env\n",
    "\n",
    "# LANGSMITH_API_KEY=<API_KEY>\n",
    "# LANGSMITH_TRACING=true\n",
    "# LANGCHAIN_PROJECT=<NOMBRE_DEL_PROYECTO>\n",
    "# LANGCHAIN_TRACING_V2=true"
   ],
   "id": "b5c170f49567c6ee",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Con las variables en el archivo .env, puedes ejecutar todo el código y cada traza será enviada a LangSmith. Sin embargo si quieres tener más control de cada traza (por ejemplo ponerle un nombre a cada uno y mejorar el monitoreo), puedes hacerlo de la siguiente manera:",
   "id": "b4b6434c2675bb46"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from langsmith import trace\n",
    "from langsmith import Client, traceable"
   ],
   "id": "45f02dc1e4bf5c0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "langsmith_client = Client(\n",
    "    api_key=os.getenv(\"LANGSMITH_API_KEY\"),\n",
    "    api_url=\"https://api.smith.langchain.com\",\n",
    ")"
   ],
   "id": "d2d637aa52da62e8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "question = \"Qué tipo de viviendas usan el pueblo añú?. Además, cita la fuente\"\n",
    "with trace(\"Chat Pipeline\", \"chain\", project_name=\"Q&A chatbot 2\", inputs={\"input\": question}, client=langsmith_client) as rt:\n",
    "    answer = execute_chain(rag_chain, question)\n",
    "    print(answer)\n",
    "    rt.end(outputs={\"output\": answer})"
   ],
   "id": "b00e9d054811c1b0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Creando un Chatbot QA con RetrievalQA",
   "id": "879bfa9ee460ffe3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-10T20:57:59.976916Z",
     "start_time": "2024-12-10T20:57:59.825018Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "rag_chain = RetrievalQA.from_chain_type(\n",
    "   llm=llm,\n",
    "   chain_type=\"stuff\",\n",
    "   retriever=retriever,\n",
    "   return_source_documents=False\n",
    ")"
   ],
   "id": "bb183f2106a7116c",
   "outputs": [],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-10T21:00:15.391071Z",
     "start_time": "2024-12-10T21:00:15.389106Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Vamos a definir de nuevo la función de ejecución de la cadena, para eliminar el retorno de answer.content, pues con RetrievalQA daría error\n",
    "\n",
    "def execute_chain(chain, question):\n",
    "    answer = chain.invoke(question)\n",
    "    return answer"
   ],
   "id": "2f40df645eb6991",
   "outputs": [],
   "execution_count": 34
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-10T21:00:20.617072Z",
     "start_time": "2024-12-10T21:00:18.249757Z"
    }
   },
   "cell_type": "code",
   "source": [
    "question = \"Qué tipo de viviendas usan el pueblo añú?. Además, cita la fuente\"\n",
    "with trace(\"RetrievalQA\", \"chain\", project_name=\"Q&A chatbot 2\", inputs={\"input\": question}, client=langsmith_client) as rt:\n",
    "    answer = execute_chain(rag_chain, question)\n",
    "    print(answer)\n",
    "    rt.end(outputs={\"output\": answer})"
   ],
   "id": "915991b18e9da0bf",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'query': 'Qué tipo de viviendas usan el pueblo añú?. Además, cita la fuente', 'result': 'El pueblo Añú utiliza palafitos como tipo de vivienda. Según el texto, \"los palafitos son modelos de viviendas, creadas por el ingenio de ésta etnia\". La fuente es La Salle, 1983, p. 20.'}\n"
     ]
    }
   ],
   "execution_count": 35
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-10T21:01:34.999624Z",
     "start_time": "2024-12-10T21:01:34.997473Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Puedes configurar el retriever de manera más avanzada, por ejemplo, para buscar solo documentos con una puntuación de similitud superior a 0.8 y devolver los 3 documentos más relevantes\n",
    "\n",
    "retriever = vector_db.as_retriever(\n",
    "    search_type=\"similarity_score_threshold\",\n",
    "    search_kwargs={'score_threshold': 0.8, 'k': 3}\n",
    ")"
   ],
   "id": "6c8fb1fb1a5f5474",
   "outputs": [],
   "execution_count": 36
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
